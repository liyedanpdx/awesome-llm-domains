## UC Berkeley

### **Chatbot Arena LLM Leaderboard: Community-driven Evaluation for Best LLM and AI chatbots**

[Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots](https://lmarena.ai/?leaderboard)

> Maintained by researchers at UC Berkeley [SkyLab](https://sky.cs.berkeley.edu/) and [LMArena](https://blog.lmarena.ai/about/)
> 

### **Berkeley Function-Calling Leaderboard**

[Berkeley Function Calling Leaderboard V3 (aka Berkeley Tool Calling Leaderboard V3)](https://gorilla.cs.berkeley.edu/leaderboard.html)

> The Berkeley Function Calling Leaderboard V3 (also called Berkeley Tool Calling Leaderboard V3) evaluates the LLM's ability to call functions (aka tools) accurately. This leaderboard consists of real-world data and will be updated periodically. For more information on the evaluation dataset and methodology, please refer to our blogs: [BFCL-v1](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html) introducing AST as an evaluation metric, [BFCL-v2](https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html) introducing enterprise and OSS-contributed functions, and [BFCL-v3](https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html) introducing multi-turn interactions. Checkout [code and data](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard).
> 

## Coding Evaluation

1. [BigCodeBench](https://bigcode-bench.github.io/)
2. [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
3. [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
4. [CrossCodeEval](https://github.com/amazon-science/cceval)
5. [ClassEval](https://fudanselab-classeval.github.io/)
6. [CRUXEval](https://crux-eval.github.io/leaderboard.html)
7. [Code Lingua](https://codetlingua.github.io/leaderboard.html)
8. [Evo-Eval](https://evo-eval.github.io/)
9. [EffiBench](https://huggingface.co/spaces/EffiBench/effibench-leaderboard)
10. [HumanEval.jl - Julia version HumanEval with EvalPlus test cases](https://github.com/01-ai/HumanEval.jl)
11. [LiveCodeBench](https://livecodebench.github.io/leaderboard.html)
12. [MHPP](https://sparksofagi.github.io/MHPP/)
13. [NaturalCodeBench](https://github.com/THUDM/NaturalCodeBench)
14. [RepoBench](https://github.com/Leolty/repobench)
15. [SWE-bench](https://www.swebench.com/)
16. [TabbyML Leaderboard](https://leaderboard.tabbyml.com/)
17. [TestEval](https://llm4softwaretesting.github.io/)
18. https://evalplus.github.io/leaderboard.html